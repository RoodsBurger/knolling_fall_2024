{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x9-zqoqSOt7F",
        "outputId": "8c5f0760-a666-417e-8174-8cdf4909ca62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.29)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.30.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install ultralytics\n",
        "!pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "import torch.multiprocessing as mp\n",
        "import torch.cuda\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR"
      ],
      "metadata": {
        "id": "xSvKsoBFO1LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_model = None\n",
        "def init_yolo():\n",
        "    global yolo_model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    yolo_model = YOLO('yolov8x.pt')\n",
        "    yolo_model.to(device)\n",
        "    return yolo_model"
      ],
      "metadata": {
        "id": "gR4iE9-JO3Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_objects(image):\n",
        "    global yolo_model\n",
        "    if yolo_model is None:\n",
        "        init_yolo()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    image = image.convert(\"RGB\") if isinstance(image, Image.Image) else Image.open(image).convert(\"RGB\")\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        results = yolo_model(image_tensor, verbose=False)\n",
        "\n",
        "    objects = []\n",
        "\n",
        "    for det in results[0].boxes.data.cpu().numpy():\n",
        "        x1, y1, x2, y2, conf, cls = det\n",
        "        if conf < 0.3:\n",
        "            continue\n",
        "\n",
        "        obj_region = image_np[int(y1):int(y2), int(x1):int(x2)]\n",
        "        gray = cv2.cvtColor(obj_region, cv2.COLOR_RGB2GRAY)\n",
        "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if contours:\n",
        "            largest_contour = max(contours, key=cv2.contourArea)\n",
        "            if len(largest_contour) >= 5:\n",
        "                ellipse = cv2.fitEllipse(largest_contour)\n",
        "                angle = ellipse[2]\n",
        "            else:\n",
        "                rect = cv2.minAreaRect(largest_contour)\n",
        "                angle = rect[2]\n",
        "        else:\n",
        "            angle = 0\n",
        "\n",
        "        angle = angle % 180\n",
        "        if angle > 90:\n",
        "            angle -= 180\n",
        "\n",
        "        obj = {\n",
        "            'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
        "            'width': int(x2 - x1),\n",
        "            'height': int(y2 - y1),\n",
        "            'center': [(x1 + x2)/2, (y1 + y2)/2],\n",
        "            'area': (x2 - x1) * (y2 - y1),\n",
        "            'image': obj_region,\n",
        "            'class': int(cls),\n",
        "            'angle': angle\n",
        "        }\n",
        "        objects.append(obj)\n",
        "\n",
        "    return objects\n",
        "\n",
        "def rotate_image(image, angle, center=None):\n",
        "    if center is None:\n",
        "        center = (image.shape[1] // 2, image.shape[0] // 2)\n",
        "\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]),\n",
        "                            flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT,\n",
        "                            borderValue=(255, 255, 255))\n",
        "    return rotated"
      ],
      "metadata": {
        "id": "m-98Xu_WPA0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnollingDataset(Dataset):\n",
        "    def __init__(self, messy_images, tidy_images):\n",
        "        self.messy_images = messy_images\n",
        "        self.tidy_images = tidy_images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.messy_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        messy_img = self.messy_images[idx]\n",
        "        tidy_img = self.tidy_images[idx]\n",
        "\n",
        "        if isinstance(messy_img, Image.Image):\n",
        "            messy_img = messy_img.convert('RGB')\n",
        "        if isinstance(tidy_img, Image.Image):\n",
        "            tidy_img = tidy_img.convert('RGB')\n",
        "\n",
        "        messy_objects = extract_objects(messy_img)\n",
        "        tidy_objects = extract_objects(tidy_img)\n",
        "\n",
        "        pos_map = torch.zeros((1, 128, 128))\n",
        "        cls_map = torch.zeros((80, 128, 128))\n",
        "        rot_map = torch.zeros((1, 128, 128))\n",
        "\n",
        "        for obj in messy_objects:\n",
        "            x1, y1, x2, y2 = obj['bbox']\n",
        "            x1, x2 = max(0, min(x1, 128)), max(0, min(x2, 128))\n",
        "            y1, y2 = max(0, min(y1, 128)), max(0, min(y2, 128))\n",
        "            if x2 > x1 and y2 > y1:\n",
        "                pos_map[0, y1:y2, x1:x2] = 1\n",
        "                cls_map[obj['class'], y1:y2, x1:x2] = 1\n",
        "                rot_map[0, y1:y2, x1:x2] = obj['angle'] / 180.0\n",
        "\n",
        "        target_pos_map = torch.zeros((1, 128, 128))\n",
        "        target_rot_map = torch.zeros((1, 128, 128))\n",
        "\n",
        "        for obj in tidy_objects:\n",
        "            x1, y1, x2, y2 = obj['bbox']\n",
        "            x1, x2 = max(0, min(x1, 128)), max(0, min(x2, 128))\n",
        "            y1, y2 = max(0, min(y1, 128)), max(0, min(y2, 128))\n",
        "            if x2 > x1 and y2 > y1:\n",
        "                target_pos_map[0, y1:y2, x1:x2] = 1\n",
        "                target_rot_map[0, y1:y2, x1:x2] = obj['angle'] / 180.0\n",
        "\n",
        "        return {\n",
        "            'position': pos_map,\n",
        "            'classes': cls_map,\n",
        "            'rotation': rot_map,\n",
        "            'target_position': target_pos_map,\n",
        "            'target_rotation': target_rot_map,\n",
        "            'messy_objects': messy_objects,\n",
        "            'tidy_objects': tidy_objects\n",
        "        }"
      ],
      "metadata": {
        "id": "ahvlqhfcPA4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(1, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(82, 128, 3, padding=1),\n",
        "                nn.GroupNorm(8, 128),\n",
        "                nn.GELU(),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(256, 256, 3, padding=1),\n",
        "                nn.GroupNorm(8, 256),\n",
        "                nn.GELU(),\n",
        "                nn.Conv2d(256, 256, 3, padding=1),\n",
        "                nn.GroupNorm(8, 256),\n",
        "                nn.GELU(),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(256, 256, 3, padding=1, stride=2),\n",
        "                nn.GroupNorm(8, 256),\n",
        "                nn.GELU(),\n",
        "                nn.Conv2d(256, 256, 3, padding=1),\n",
        "                nn.GroupNorm(8, 256),\n",
        "                nn.GELU(),\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(256, 8, batch_first=True)\n",
        "        self.decoder = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose2d(256, 256, 4, stride=2, padding=1),\n",
        "                nn.GroupNorm(8, 256),\n",
        "                nn.GELU(),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(512, 256, 3, padding=1),\n",
        "                nn.GroupNorm(8, 256),\n",
        "                nn.GELU(),\n",
        "                nn.Conv2d(256, 128, 3, padding=1),\n",
        "                nn.GroupNorm(8, 128),\n",
        "                nn.GELU(),\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.position_head = nn.Sequential(\n",
        "            nn.Conv2d(256, 64, 3, padding=1),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(64, 1, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.rotation_head = nn.Sequential(\n",
        "            nn.Conv2d(256, 64, 3, padding=1),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(64, 1, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        t = t.view(batch_size, 1)\n",
        "        t_embed = self.time_embed(t)\n",
        "        t_embed = t_embed.view(batch_size, -1, 1, 1).expand(-1, -1, x.shape[2], x.shape[3])\n",
        "\n",
        "        skip_connections = []\n",
        "\n",
        "        x = self.encoder[0](x)\n",
        "        skip_connections.append(x)\n",
        "\n",
        "        x = torch.cat([x, t_embed], dim=1)\n",
        "\n",
        "        x = self.encoder[1](x)\n",
        "        skip_connections.append(x)\n",
        "        x = self.encoder[2](x)\n",
        "\n",
        "        b, c, h, w = x.shape\n",
        "        x_flat = x.flatten(2).permute(0, 2, 1)\n",
        "        x_flat, _ = self.attention(x_flat, x_flat, x_flat)\n",
        "        x = x_flat.permute(0, 2, 1).view(b, c, h, w)\n",
        "\n",
        "        x = self.decoder[0](x)\n",
        "        x = torch.cat([x, skip_connections.pop()], dim=1)\n",
        "\n",
        "        x = self.decoder[1](x)\n",
        "        x = torch.cat([x, skip_connections.pop()], dim=1)\n",
        "\n",
        "        position = self.position_head(x)\n",
        "        rotation = self.rotation_head(x)\n",
        "\n",
        "        return position, rotation"
      ],
      "metadata": {
        "id": "pWbDYc5uPA7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_detection_loss(pred, target):\n",
        "    def sobel(x):\n",
        "        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], device=x.device).float()\n",
        "        kernel_y = kernel_x.t()\n",
        "        kernel_x = kernel_x.view(1, 1, 3, 3).repeat(1, 1, 1, 1)\n",
        "        kernel_y = kernel_y.view(1, 1, 3, 3).repeat(1, 1, 1, 1)\n",
        "\n",
        "        grad_x = F.conv2d(x, kernel_x, padding=1)\n",
        "        grad_y = F.conv2d(x, kernel_y, padding=1)\n",
        "\n",
        "        return torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)\n",
        "\n",
        "    pred_edges = sobel(pred)\n",
        "    target_edges = sobel(target)\n",
        "\n",
        "    return F.mse_loss(pred_edges, target_edges)\n",
        "\n",
        "def relative_position_loss(pred, target):\n",
        "    def get_centroids(x):\n",
        "        b, c, h, w = x.shape\n",
        "        y_coords = torch.arange(h, device=x.device).float().view(1, 1, -1, 1)\n",
        "        x_coords = torch.arange(w, device=x.device).float().view(1, 1, 1, -1)\n",
        "\n",
        "        y_centroid = torch.sum(y_coords * x, dim=(2, 3)) / (torch.sum(x, dim=(2, 3)) + 1e-6)\n",
        "        x_centroid = torch.sum(x_coords * x, dim=(2, 3)) / (torch.sum(x, dim=(2, 3)) + 1e-6)\n",
        "\n",
        "        return y_centroid, x_centroid\n",
        "\n",
        "    pred_y, pred_x = get_centroids(pred)\n",
        "    target_y, target_x = get_centroids(target)\n",
        "\n",
        "    return F.mse_loss(pred_y, target_y) + F.mse_loss(pred_x, target_x)\n",
        "\n",
        "def custom_collate(batch):\n",
        "    elem = batch[0]\n",
        "    collated = {}\n",
        "\n",
        "    for key in ['position', 'classes', 'rotation', 'target_position', 'target_rotation']:\n",
        "        if key in elem:\n",
        "            collated[key] = default_collate([d[key] for d in batch])\n",
        "\n",
        "    collated['messy_objects'] = [d['messy_objects'] for d in batch]\n",
        "    collated['tidy_objects'] = [d['tidy_objects'] for d in batch]\n",
        "\n",
        "    return collated\n",
        "\n",
        "def spatial_consistency_loss(pred, target, messy_objects):\n",
        "    device = pred.device\n",
        "    batch_size = pred.shape[0]\n",
        "    total_loss = torch.tensor(0., device=device, requires_grad=True)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        if len(messy_objects[b]) < 2:\n",
        "            continue\n",
        "\n",
        "        centers_pred = []\n",
        "        centers_target = []\n",
        "\n",
        "        pred_map = pred[b, 0].detach().cpu().numpy()\n",
        "        target_map = target[b, 0].detach().cpu().numpy()\n",
        "\n",
        "        num_labels_pred, labels_pred = cv2.connectedComponents((pred_map > 0.5).astype(np.uint8))\n",
        "        num_labels_target, labels_target = cv2.connectedComponents((target_map > 0.5).astype(np.uint8))\n",
        "\n",
        "        for i in range(1, num_labels_pred):\n",
        "            mask = labels_pred == i\n",
        "            if np.sum(mask) > 0:\n",
        "                y, x = np.where(mask)\n",
        "                centers_pred.append([np.mean(x), np.mean(y)])\n",
        "\n",
        "        for i in range(1, num_labels_target):\n",
        "            mask = labels_target == i\n",
        "            if np.sum(mask) > 0:\n",
        "                y, x = np.where(mask)\n",
        "                centers_target.append([np.mean(x), np.mean(y)])\n",
        "\n",
        "        if len(centers_pred) < 2 or len(centers_target) < 2:\n",
        "            continue\n",
        "\n",
        "        centers_pred = torch.tensor(centers_pred, device=device, requires_grad=True)\n",
        "        centers_target = torch.tensor(centers_target, device=device)\n",
        "\n",
        "        min_centers = min(len(centers_pred), len(centers_target))\n",
        "        centers_pred = centers_pred[:min_centers]\n",
        "        centers_target = centers_target[:min_centers]\n",
        "\n",
        "        dist_pred = torch.cdist(centers_pred, centers_pred)\n",
        "        dist_target = torch.cdist(centers_target, centers_target)\n",
        "\n",
        "        dist_pred = dist_pred / (torch.max(dist_pred) + 1e-6)\n",
        "        dist_target = dist_target / (torch.max(dist_target) + 1e-6)\n",
        "\n",
        "        total_loss = total_loss + F.mse_loss(dist_pred, dist_target)\n",
        "\n",
        "    return total_loss / max(batch_size, 1)\n",
        "\n",
        "def alignment_loss(pred):\n",
        "    device = pred.device\n",
        "    batch_size = pred.shape[0]\n",
        "    total_loss = torch.tensor(0., device=device, requires_grad=True)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        pred_map = pred[b, 0]\n",
        "\n",
        "        grad_y = pred_map[1:, :] - pred_map[:-1, :]\n",
        "        grad_x = pred_map[:, 1:] - pred_map[:, :-1]\n",
        "\n",
        "        grad_y_var = torch.var(grad_y[grad_y != 0]) if torch.any(grad_y != 0) else torch.tensor(0., device=device, requires_grad=True)\n",
        "        grad_x_var = torch.var(grad_x[grad_x != 0]) if torch.any(grad_x != 0) else torch.tensor(0., device=device, requires_grad=True)\n",
        "\n",
        "        total_loss = total_loss + grad_y_var + grad_x_var\n",
        "\n",
        "    return total_loss / max(batch_size, 1)\n",
        "\n",
        "def train_model(model, dataset, num_epochs=10, batch_size=4):\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-5,\n",
        "        weight_decay=0.001,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    steps_per_epoch = len(DataLoader(dataset, batch_size=batch_size))\n",
        "\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=1e-4,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        pct_start=0.3,\n",
        "        div_factor=25.0,\n",
        "        final_div_factor=1000.0\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=custom_collate,\n",
        "        num_workers=0,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Training on device: {device}\")\n",
        "    print(f\"Number of batches: {len(dataloader)}\")\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "\n",
        "    moving_avg_loss = None\n",
        "    alpha = 0.9\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        epoch_losses = {'mse_pos': 0, 'mse_rot': 0, 'edge': 0, 'spatial': 0, 'align': 0}\n",
        "        num_batches = 0\n",
        "\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = torch.cat([batch['position'], batch['classes'], batch['rotation']], dim=1).to(device)\n",
        "            target_pos = batch['target_position'].to(device)\n",
        "            target_rot = batch['target_rotation'].to(device)\n",
        "\n",
        "            noise_scale = max(0.05, 0.5 * (1.0 - (epoch / num_epochs)))\n",
        "            t = torch.rand(x.shape[0], device=device) * noise_scale\n",
        "            noise = torch.randn_like(x) * 0.1\n",
        "            noised_x = x + noise * t.view(-1, 1, 1, 1)\n",
        "            pred_pos, pred_rot = model(noised_x, t)\n",
        "\n",
        "            if i == 0 and epoch == 0:\n",
        "                print(f\"Input shape: {x.shape}\")\n",
        "                print(f\"Predicted position shape: {pred_pos.shape}\")\n",
        "                print(f\"Predicted rotation shape: {pred_rot.shape}\")\n",
        "                print(f\"Target position shape: {target_pos.shape}\")\n",
        "                print(f\"Target rotation shape: {target_rot.shape}\")\n",
        "\n",
        "            mse_loss_pos = nn.MSELoss()(pred_pos, target_pos)\n",
        "            mse_loss_rot = nn.MSELoss()(pred_rot, target_rot)\n",
        "            edge_loss = edge_detection_loss(pred_pos, target_pos)\n",
        "            spatial_loss = spatial_consistency_loss(pred_pos, target_pos, batch['messy_objects'])\n",
        "            align_loss = alignment_loss(pred_pos)\n",
        "\n",
        "            normalized_edge_loss = edge_loss / (edge_loss.detach() + 1e-8)\n",
        "            normalized_spatial_loss = spatial_loss / (spatial_loss.detach() + 1e-8)\n",
        "            normalized_align_loss = align_loss / (align_loss.detach() + 1e-8)\n",
        "\n",
        "            loss = (\n",
        "                mse_loss_pos +\n",
        "                0.5 * mse_loss_rot +\n",
        "                0.05 * normalized_edge_loss +\n",
        "                0.1 * normalized_spatial_loss +\n",
        "                0.05 * normalized_align_loss\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            current_loss = loss.item()\n",
        "            if moving_avg_loss is None:\n",
        "                moving_avg_loss = current_loss\n",
        "            else:\n",
        "                moving_avg_loss = alpha * moving_avg_loss + (1 - alpha) * current_loss\n",
        "\n",
        "            total_loss += current_loss\n",
        "            epoch_losses['mse_pos'] += mse_loss_pos.item()\n",
        "            epoch_losses['mse_rot'] += mse_loss_rot.item()\n",
        "            epoch_losses['edge'] += edge_loss.item()\n",
        "            epoch_losses['spatial'] += spatial_loss.item()\n",
        "            epoch_losses['align'] += align_loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}, Batch {i}, Loss: {current_loss:.4f}, MovingAvg: {moving_avg_loss:.4f}')\n",
        "                print(f'MSE Pos: {mse_loss_pos.item():.4f}, MSE Rot: {mse_loss_rot.item():.4f},')\n",
        "                print(f'Edge: {edge_loss.item():.4f}, Spatial: {spatial_loss.item():.4f}, Align: {align_loss.item():.4f}')\n",
        "                print(f'Learning rate: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}\n",
        "\n",
        "        print(f'Epoch {epoch+1}:')\n",
        "        print(f'  Average Loss: {avg_loss:.4f}')\n",
        "        print(f'  MSE Pos: {avg_losses[\"mse_pos\"]:.4f}, MSE Rot: {avg_losses[\"mse_rot\"]:.4f}')\n",
        "        print(f'  Edge: {avg_losses[\"edge\"]:.4f}')\n",
        "        print(f'  Spatial: {avg_losses[\"spatial\"]:.4f}')\n",
        "        print(f'  Align: {avg_losses[\"align\"]:.4f}')\n",
        "        print(f'  LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "            print(f'New best model saved with loss: {best_loss:.4f}')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "                break\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_loss': best_loss,\n",
        "                'moving_avg_loss': moving_avg_loss,\n",
        "            }, f'knolling_model_epoch_{epoch+1}.pth')\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Cb2DMScZPA9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tidy_layout(model, messy_image):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(messy_image, Image.Image):\n",
        "        messy_array = np.array(messy_image)\n",
        "    else:\n",
        "        messy_array = messy_image\n",
        "\n",
        "    background = messy_array.copy()\n",
        "\n",
        "    objects = extract_objects(messy_image)\n",
        "\n",
        "    if not objects:\n",
        "        return Image.fromarray(messy_array)\n",
        "\n",
        "    object_mask = np.zeros((128, 128), dtype=np.uint8)\n",
        "    for obj in objects:\n",
        "        x1, y1, x2, y2 = obj['bbox']\n",
        "        x1, x2 = max(0, min(x1, 128)), max(0, min(x2, 128))\n",
        "        y1, y2 = max(0, min(y1, 128)), max(0, min(y2, 128))\n",
        "        if x2 > x1 and y2 > y1:\n",
        "            object_mask[y1:y2, x1:x2] = 1\n",
        "\n",
        "    background_pixels = messy_array[object_mask == 0]\n",
        "    if len(background_pixels) > 0:\n",
        "        background_color = np.median(background_pixels, axis=0)\n",
        "    else:\n",
        "        background_color = np.mean(messy_array.reshape(-1, 3), axis=0)\n",
        "\n",
        "    output = np.full((128, 128, 3), background_color, dtype=np.uint8)\n",
        "\n",
        "    pos_map = torch.zeros((1, 1, 128, 128))\n",
        "    cls_map = torch.zeros((1, 80, 128, 128))\n",
        "    rot_map = torch.zeros((1, 1, 128, 128))\n",
        "\n",
        "    for obj in objects:\n",
        "        x1, y1, x2, y2 = obj['bbox']\n",
        "        x1, x2 = max(0, min(x1, 128)), max(0, min(x2, 128))\n",
        "        y1, y2 = max(0, min(y1, 128)), max(0, min(y2, 128))\n",
        "        if x2 > x1 and y2 > y1:\n",
        "            pos_map[0, 0, y1:y2, x1:x2] = 1\n",
        "            cls_map[0, obj['class'], y1:y2, x1:x2] = 1\n",
        "            rot_map[0, 0, y1:y2, x1:x2] = obj['angle'] / 180.0\n",
        "\n",
        "    x = torch.cat([pos_map, cls_map, rot_map], dim=1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        t = torch.zeros(1, device=device)\n",
        "        pred_pos, pred_rot = model(x, t)\n",
        "\n",
        "    pred_pos = pred_pos.cpu().numpy()[0, 0]\n",
        "    pred_rot = pred_rot.cpu().numpy()[0, 0] * 180.0\n",
        "\n",
        "    grid_size = int(np.sqrt(len(objects))) + 1\n",
        "    cell_size = 128 // grid_size\n",
        "\n",
        "    available_cells = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
        "\n",
        "    objects.sort(key=lambda obj: obj['width'] * obj['height'], reverse=True)\n",
        "\n",
        "    new_objects = []\n",
        "\n",
        "    for obj in objects:\n",
        "        if not available_cells:\n",
        "            break\n",
        "\n",
        "        best_cell = available_cells[0]\n",
        "        min_overlap = float('inf')\n",
        "\n",
        "        for cell in available_cells:\n",
        "            cell_x = cell[1] * cell_size\n",
        "            cell_y = cell[0] * cell_size\n",
        "\n",
        "            overlap = 0\n",
        "            for placed_obj in new_objects:\n",
        "                x1, y1, x2, y2 = placed_obj['bbox']\n",
        "                if (cell_x < x2 and cell_x + obj['width'] > x1 and\n",
        "                    cell_y < y2 and cell_y + obj['height'] > y1):\n",
        "                    overlap += 1\n",
        "\n",
        "            if overlap < min_overlap:\n",
        "                min_overlap = overlap\n",
        "                best_cell = cell\n",
        "\n",
        "        cell_x = best_cell[1] * cell_size\n",
        "        cell_y = best_cell[0] * cell_size\n",
        "\n",
        "        obj_width = min(obj['width'], cell_size)\n",
        "        obj_height = min(obj['height'], cell_size)\n",
        "\n",
        "        target_angle = pred_rot[cell_y:cell_y+obj_height, cell_x:cell_x+obj_width].mean()\n",
        "\n",
        "        new_obj = obj.copy()\n",
        "        new_obj['bbox'] = [\n",
        "            cell_x,\n",
        "            cell_y,\n",
        "            min(cell_x + obj_width, 128),\n",
        "            min(cell_y + obj_height, 128)\n",
        "        ]\n",
        "        new_obj['target_angle'] = target_angle\n",
        "        new_objects.append(new_obj)\n",
        "\n",
        "        available_cells.remove(best_cell)\n",
        "\n",
        "    for obj in new_objects:\n",
        "        x1, y1, x2, y2 = obj['bbox']\n",
        "        angle = obj['target_angle']\n",
        "        rotated_img = rotate_image(obj['image'], angle, center=(obj['center'][0]-x1, obj['center'][1]-y1))\n",
        "        output[y1:y2, x1:x2] = rotated_img[0:y2-y1, 0:x2-x1]\n",
        "\n",
        "    return Image.fromarray(output)"
      ],
      "metadata": {
        "id": "jFNTXFSOPA_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abs_path = '/content/drive/Othercomputers/MacBook Pro (Personal)/Documents/COLUMBIA UNIVERSITY/MSCS/Research/Knolling Bot/Preliminary Pipeline/'\n",
        "messy_images_dir = abs_path + 'data/images_before_small/'\n",
        "tidy_images_dir = abs_path + 'data/images_after_small/'"
      ],
      "metadata": {
        "id": "klihYMfjQKJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messy_paths = sorted(glob.glob(messy_images_dir+\"/*.png\"))\n",
        "tidy_paths = sorted(glob.glob(tidy_images_dir+\"/*.png\"))\n",
        "dataset = KnollingDataset(messy_paths, tidy_paths)"
      ],
      "metadata": {
        "id": "fSkC7cZHPBBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiffusionModel()\n",
        "init_yolo()\n",
        "trained_model = train_model(model, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13K6MHGvPBDF",
        "outputId": "bdb67c72-a703-4dca-fda4-683c4546a64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "Number of batches: 27\n",
            "Input shape: torch.Size([4, 82, 128, 128])\n",
            "Predicted position shape: torch.Size([4, 1, 128, 128])\n",
            "Predicted rotation shape: torch.Size([4, 1, 128, 128])\n",
            "Target position shape: torch.Size([4, 1, 128, 128])\n",
            "Target rotation shape: torch.Size([4, 1, 128, 128])\n",
            "Epoch 1, Batch 0, Loss: 0.4909, MovingAvg: 0.4909\n",
            "MSE Pos: 0.2610, MSE Rot: 0.0597,\n",
            "Edge: 0.6985, Spatial: 0.1326, Align: 0.0132\n",
            "Learning rate: 0.000004\n",
            "Epoch 1, Batch 10, Loss: 0.3572, MovingAvg: 0.4378\n",
            "MSE Pos: 0.1420, MSE Rot: 0.0304,\n",
            "Edge: 0.4158, Spatial: 0.0056, Align: 0.0040\n",
            "Learning rate: 0.000008\n",
            "Epoch 1, Batch 20, Loss: 0.2022, MovingAvg: 0.3139\n",
            "MSE Pos: 0.0928, MSE Rot: 0.0188,\n",
            "Edge: 0.4413, Spatial: 0.0000, Align: 0.0008\n",
            "Learning rate: 0.000019\n",
            "Epoch 1:\n",
            "  Average Loss: 0.3127\n",
            "  MSE Pos: 0.1466, MSE Rot: 0.0286\n",
            "  Edge: 0.5265\n",
            "  Spatial: 0.0264\n",
            "  Align: 0.0035\n",
            "  LR: 0.000029\n",
            "New best model saved with loss: 0.3127\n",
            "Epoch 2, Batch 0, Loss: 0.1959, MovingAvg: 0.2491\n",
            "MSE Pos: 0.0874, MSE Rot: 0.0169,\n",
            "Edge: 0.5230, Spatial: 0.0000, Align: 0.0007\n",
            "Learning rate: 0.000030\n",
            "Epoch 2, Batch 10, Loss: 0.1943, MovingAvg: 0.2066\n",
            "MSE Pos: 0.0871, MSE Rot: 0.0145,\n",
            "Edge: 0.5925, Spatial: 0.0000, Align: 0.0003\n",
            "Learning rate: 0.000048\n",
            "Epoch 2, Batch 20, Loss: 0.1335, MovingAvg: 0.1783\n",
            "MSE Pos: 0.0318, MSE Rot: 0.0035,\n",
            "Edge: 0.1727, Spatial: 0.0000, Align: 0.0001\n",
            "Learning rate: 0.000067\n",
            "Epoch 2:\n",
            "  Average Loss: 0.1719\n",
            "  MSE Pos: 0.0666, MSE Rot: 0.0106\n",
            "  Edge: 0.4562\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0003\n",
            "  LR: 0.000077\n",
            "New best model saved with loss: 0.1719\n",
            "Epoch 3, Batch 0, Loss: 0.1765, MovingAvg: 0.1708\n",
            "MSE Pos: 0.0713, MSE Rot: 0.0103,\n",
            "Edge: 0.5955, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000079\n",
            "Epoch 3, Batch 10, Loss: 0.1505, MovingAvg: 0.1654\n",
            "MSE Pos: 0.0482, MSE Rot: 0.0047,\n",
            "Edge: 0.3734, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000092\n",
            "Epoch 3, Batch 20, Loss: 0.2008, MovingAvg: 0.1647\n",
            "MSE Pos: 0.0962, MSE Rot: 0.0097,\n",
            "Edge: 0.6895, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000099\n",
            "Epoch 3:\n",
            "  Average Loss: 0.1632\n",
            "  MSE Pos: 0.0600, MSE Rot: 0.0067\n",
            "  Edge: 0.4497\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0000\n",
            "  LR: 0.000100\n",
            "New best model saved with loss: 0.1632\n",
            "Epoch 4, Batch 0, Loss: 0.1497, MovingAvg: 0.1629\n",
            "MSE Pos: 0.0471, MSE Rot: 0.0069,\n",
            "Edge: 0.3459, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000100\n",
            "Epoch 4, Batch 10, Loss: 0.1548, MovingAvg: 0.1613\n",
            "MSE Pos: 0.0533, MSE Rot: 0.0052,\n",
            "Edge: 0.4127, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000099\n",
            "Epoch 4, Batch 20, Loss: 0.1578, MovingAvg: 0.1622\n",
            "MSE Pos: 0.0586, MSE Rot: 0.0016,\n",
            "Edge: 0.5207, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000097\n",
            "Epoch 4:\n",
            "  Average Loss: 0.1584\n",
            "  MSE Pos: 0.0571, MSE Rot: 0.0057\n",
            "  Edge: 0.4366\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0000\n",
            "  LR: 0.000095\n",
            "New best model saved with loss: 0.1584\n",
            "Epoch 5, Batch 0, Loss: 0.1856, MovingAvg: 0.1573\n",
            "MSE Pos: 0.0816, MSE Rot: 0.0108,\n",
            "Edge: 0.6783, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000094\n",
            "Epoch 5, Batch 10, Loss: 0.1924, MovingAvg: 0.1597\n",
            "MSE Pos: 0.0938, MSE Rot: 0.0115,\n",
            "Edge: 0.6541, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000090\n",
            "Epoch 5, Batch 20, Loss: 0.1463, MovingAvg: 0.1563\n",
            "MSE Pos: 0.0509, MSE Rot: 0.0014,\n",
            "Edge: 0.4911, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000084\n",
            "Epoch 5:\n",
            "  Average Loss: 0.1570\n",
            "  MSE Pos: 0.0590, MSE Rot: 0.0054\n",
            "  Edge: 0.4534\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0000\n",
            "  LR: 0.000081\n",
            "New best model saved with loss: 0.1570\n",
            "Epoch 6, Batch 0, Loss: 0.1402, MovingAvg: 0.1554\n",
            "MSE Pos: 0.0458, MSE Rot: 0.0005,\n",
            "Edge: 0.4566, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000080\n",
            "Epoch 6, Batch 10, Loss: 0.1207, MovingAvg: 0.1544\n",
            "MSE Pos: 0.0274, MSE Rot: 0.0024,\n",
            "Edge: 0.1742, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000073\n",
            "Epoch 6, Batch 20, Loss: 0.1298, MovingAvg: 0.1510\n",
            "MSE Pos: 0.0391, MSE Rot: 0.0032,\n",
            "Edge: 0.3770, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000065\n",
            "Epoch 6:\n",
            "  Average Loss: 0.1496\n",
            "  MSE Pos: 0.0574, MSE Rot: 0.0053\n",
            "  Edge: 0.4436\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0000\n",
            "  LR: 0.000060\n",
            "New best model saved with loss: 0.1496\n",
            "Epoch 7, Batch 0, Loss: 0.1448, MovingAvg: 0.1445\n",
            "MSE Pos: 0.0579, MSE Rot: 0.0017,\n",
            "Edge: 0.4399, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000060\n",
            "Epoch 7, Batch 10, Loss: 0.1230, MovingAvg: 0.1427\n",
            "MSE Pos: 0.0508, MSE Rot: 0.0058,\n",
            "Edge: 0.2940, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000051\n",
            "Epoch 7, Batch 20, Loss: 0.1397, MovingAvg: 0.1463\n",
            "MSE Pos: 0.0547, MSE Rot: 0.0035,\n",
            "Edge: 0.5047, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000043\n",
            "Epoch 7:\n",
            "  Average Loss: 0.1421\n",
            "  MSE Pos: 0.0567, MSE Rot: 0.0051\n",
            "  Edge: 0.4361\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0000\n",
            "  LR: 0.000038\n",
            "New best model saved with loss: 0.1421\n",
            "Epoch 8, Batch 0, Loss: 0.1079, MovingAvg: 0.1338\n",
            "MSE Pos: 0.0204, MSE Rot: 0.0002,\n",
            "Edge: 0.1876, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000037\n",
            "Epoch 8, Batch 10, Loss: 0.1555, MovingAvg: 0.1377\n",
            "MSE Pos: 0.0768, MSE Rot: 0.0078,\n",
            "Edge: 0.6458, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000029\n",
            "Epoch 8, Batch 20, Loss: 0.1787, MovingAvg: 0.1388\n",
            "MSE Pos: 0.1013, MSE Rot: 0.0058,\n",
            "Edge: 0.5857, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000022\n",
            "Epoch 8:\n",
            "  Average Loss: 0.1374\n",
            "  MSE Pos: 0.0589, MSE Rot: 0.0051\n",
            "  Edge: 0.4586\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0000\n",
            "  LR: 0.000018\n",
            "New best model saved with loss: 0.1374\n",
            "Epoch 9, Batch 0, Loss: 0.1175, MovingAvg: 0.1375\n",
            "MSE Pos: 0.0474, MSE Rot: 0.0037,\n",
            "Edge: 0.2649, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000018\n",
            "Epoch 9, Batch 10, Loss: 0.1195, MovingAvg: 0.1278\n",
            "MSE Pos: 0.0531, MSE Rot: 0.0021,\n",
            "Edge: 0.4450, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000012\n",
            "Epoch 9, Batch 20, Loss: 0.1336, MovingAvg: 0.1274\n",
            "MSE Pos: 0.0637, MSE Rot: 0.0086,\n",
            "Edge: 0.5131, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000007\n",
            "Epoch 9:\n",
            "  Average Loss: 0.1281\n",
            "  MSE Pos: 0.0602, MSE Rot: 0.0050\n",
            "  Edge: 0.4546\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0000\n",
            "  LR: 0.000005\n",
            "New best model saved with loss: 0.1281\n",
            "Epoch 10, Batch 0, Loss: 0.1198, MovingAvg: 0.1320\n",
            "MSE Pos: 0.0587, MSE Rot: 0.0052,\n",
            "Edge: 0.4483, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000004\n",
            "Epoch 10, Batch 10, Loss: 0.1308, MovingAvg: 0.1278\n",
            "MSE Pos: 0.0699, MSE Rot: 0.0033,\n",
            "Edge: 0.5423, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000002\n",
            "Epoch 10, Batch 20, Loss: 0.1347, MovingAvg: 0.1191\n",
            "MSE Pos: 0.0721, MSE Rot: 0.0058,\n",
            "Edge: 0.5188, Spatial: 0.0000, Align: 0.0000\n",
            "Learning rate: 0.000000\n",
            "Epoch 10:\n",
            "  Average Loss: 0.1210\n",
            "  MSE Pos: 0.0588, MSE Rot: 0.0051\n",
            "  Edge: 0.4553\n",
            "  Spatial: 0.0000\n",
            "  Align: 0.0000\n",
            "  LR: 0.000000\n",
            "New best model saved with loss: 0.1210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messy_images = [Image.open(os.path.join(messy_images_dir, filename)) for filename in sorted(os.listdir(messy_images_dir)) if filename.endswith(('.png', '.jpg'))]\n",
        "tidy_images = [Image.open(os.path.join(tidy_images_dir, filename)) for filename in sorted(os.listdir(tidy_images_dir)) if filename.endswith(('.png', '.jpg'))]"
      ],
      "metadata": {
        "id": "0ZKjn4mTPBHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tidy_image = generate_tidy_layout(trained_model, messy_images[1])"
      ],
      "metadata": {
        "id": "0j2mTTiZPBFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messy_images[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "8GZaTDcIav9-",
        "outputId": "446c13a4-0546-406e-ffc3-ea28ef70c506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=128x128>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tidy_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "ZYUo6k-layYe",
        "outputId": "a94ad08e-5627-435a-8427-35d85f795794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tidy_images[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "FoqKO61Ja4EL",
        "outputId": "431ec04f-47c8-42a7-eea9-329e86fb2c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=128x128>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rI0Q-Wmjaiu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}